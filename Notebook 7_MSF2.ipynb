{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Finance with Python\n",
    "\n",
    "### Alan Moreira, University of Rochester Simon Graduate School of Business\n",
    "\n",
    "# Notebook 7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics covered\n",
    "* * *\n",
    " * Implementing trading strategies using stock level data\n",
    " * Examples: \n",
    "    - Size anomaly\n",
    "    - Momentum\n",
    "    - Reversals\n",
    "    - Low volatility\n",
    " * Implementation trade-offs\n",
    " \n",
    "   - Liquidity\n",
    "   \n",
    "   - Tracking error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wrds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9b77eaced928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwrds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wrds'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import wrds\n",
    "import psycopg2 \n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import *\n",
    "from pandas.tseries.offsets import *\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download data from WRDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## GET DATA\n",
    "\n",
    "# WE will connect with WRDS a data provider aggregator. You can get a password to it by using your rochester email\n",
    "\n",
    "# we will get data from CRSP , the leading data provider on historical return data. Data is substantially cleaner \n",
    "#than bloomberg which tends to have a lot of mistakes especially pre 2000\n",
    "\n",
    "\n",
    "###################\n",
    "# Connect to WRDS #\n",
    "###################\n",
    "wrdsdo=True # if false get pre downloaded data from dropbox \n",
    "if wrdsdo:\n",
    "    conn=wrds.Connection()\n",
    "\n",
    "    ###################\n",
    "    # CRSP Block      #\n",
    "    ###################\n",
    "    # The CRSP data set is where you get return data. Here we are tapping into different databasis. Ons is CRSP.msf, \n",
    "    #which has monthly retruns, prices, share outstanding. Information about the different fields can be found at:\n",
    "    # https://wrds-web.wharton.upenn.edu/wrds/ds/crsp/stock_a/msf.cfm?navId=128 \n",
    "\n",
    "    # the second database in CRSP is CRSP.msenames which has information about the share code, cusip, exchange code, sector\n",
    "    # of the firm - basically information that is fixed for a given security\n",
    "\n",
    "    # this code below merges both these data sets using the permno identifier and the criteria that the dates match\n",
    "    crsp_mo = conn.raw_sql(\"\"\"\n",
    "                      select a.permno, a.permco,a.ret, a.vol, a.shrout, a.prc, a.cfacpr, a.cfacshr, a.date, \n",
    "                      b.shrcd, b.exchcd, b.siccd, b.ncusip\n",
    "                      from crsp.msf as a\n",
    "                      left join crsp.msenames as b\n",
    "                      on a.permno=b.permno\n",
    "                      and b.namedt<=a.date\n",
    "                      and a.date<=b.nameendt\n",
    "                      where a.date between '01/01/1990' and '12/31/2017'\n",
    "                      and b.shrcd between 10 and 11\n",
    "                      \"\"\") \n",
    "    #crsp_mo.to_csv('E:/Dropbox/Public/Fin418/Data/crsp_monthly.csv')\n",
    "else:\n",
    "    # Pick one\n",
    "    # this is larger data set starting in 1963\n",
    "  #  url='https://www.dropbox.com/s/5ztd2mdq6dn7cqj/crsp19632017.csv?dl=1'\n",
    "    # this is a little smalled starting in  1990\n",
    "    url='https://www.dropbox.com/s/bwjtow60uhdg0vc/crsp19902017.csv?dl=1'\n",
    "    crsp_m0=pd.read_csv(url,index_col=0)\n",
    "    \n",
    "# variables downloaded\n",
    "\n",
    "# 1. Permno, Permco, ncusip, different types of firm identifiers-- some are firm specific other security specific\n",
    "\n",
    "# 2. shrco is the type of share: common share, ADR, ETF, ....we will focus on common shares\n",
    "\n",
    "# 3. exchcd is the code of the exchange where the stock was originally listed\n",
    "\n",
    "# 4. siccd: Industry code of the firm\n",
    "\n",
    "# 5. ret, vol, shrout, and prc, are the stock return, trading volume, number of share outstanding, and price\n",
    "\n",
    "# 6.cfacpr: Cumulative Factor to Adjust Price, allow you to adjust prices for splits \n",
    "#   cfacshr: Cumulative Factor to Adjust SHARES, allow you to adjust Shares outstanding for splits \n",
    "\n",
    "\n",
    "#7. date is the trading date of the return\n",
    "\n",
    "#8 .NAMEENDT is a the last effective date of a security's name history structure. \n",
    "#It is set to the date preceding the Name Effective Date of the next name structure, the maximum of End of Stock Data, \n",
    "#or the Delisting Date of the last name structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "crsp_mo.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "- Permco is what we will use as the true identifier of a firm, as the others tend to change\n",
    "- Need to be careful with prices as negative prices have \"meaning\". Basically means that there was no trading in the closing auction and they are simply an average of the bid and the ask.\n",
    "- Need to be careful with the units of share outstanding. They are in multiples of 1000\n",
    "- Some firms have multiple securities, need to aggregate across the securities as strategies typically don't have a view on which particualr security is better to make a bet on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crsp_mo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize data set and create market value of equity for each firm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# organize CRSP and create market value of equity for each firm\n",
    "\n",
    "\n",
    "crsp_m=crsp_mo.copy()# create copy so we do not change the original file\n",
    "crsp_m[['permco','permno','shrcd','exchcd']]=\\\n",
    "    crsp_m[['permco','permno','shrcd','exchcd']].astype(int)# change variable format to integer\n",
    "crsp_m['date']=pd.to_datetime(crsp_m['date']) # convert to date\n",
    "\n",
    "\n",
    "# construction of total firm marke cap (need if you want to value weight)\n",
    "crsp_m['jdate']=crsp_m['date']+MonthEnd(0) # Line up date to be end of month\n",
    "crsp_m['p']=crsp_m['prc'].abs()/crsp_m['cfacpr'] # price adjusted to splits, absolute value to deal with \"negative\" prices\n",
    "crsp_m['tso']=crsp_m['shrout']*crsp_m['cfacshr']*1e3 # total shares out adjusted- reported in thousands\n",
    "crsp_m['me'] = crsp_m['p']*crsp_m['tso']/1e6 # market cap in $mil\n",
    "\n",
    "crsp_summe = crsp_m.groupby(['jdate','permco'])['me'].sum().reset_index()\\\n",
    "    .rename(columns={'me':'me_comp'})\n",
    "# sum of me across different permno belonging to same permco a given date\n",
    "crsp_m=pd.merge(crsp_m, crsp_summe, how='inner', on=['jdate','permco'])# merge back with the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "crsp_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size anomaly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#1. make sure is properly sorted\n",
    "crsp_m.sort_values(['permco','date'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Keep only what we need\n",
    "\n",
    "df=crsp_m[['date','permco','me_comp','ret']].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Lag sorting variable by 1 month\n",
    "\n",
    "df['me1']=df.groupby('permco')['me_comp'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Create portfolios based on size sorts\n",
    "\n",
    "\n",
    "df['group']=df.groupby(['date'])['me1'].transform(lambda x: pd.qcut(x, 10, labels=False,duplicates='drop'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# portfolio membership is done. Simply need to use weighting scheme\n",
    "\n",
    "def wavg(group):\n",
    "    d = group['ret']\n",
    "    w = group['me1']\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "# We now simply have to  use the above function to construct the portfolio\n",
    "\n",
    "# the code below applies the function in each date,mom_group group,\n",
    "# so it applies the function to each of these subgroups of the data set, \n",
    "# so it retursn one time-series for each mom_group, as it average the returns of\n",
    "# all the firms in a given group in a given date\n",
    "port_vwret = df.groupby(['date','group']).apply(wavg)\n",
    "port_vwret.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "port_vwret = port_vwret.reset_index().rename(columns={0:'port_vwret'})# give a name to the new time-seires\n",
    "port_vwret=port_vwret.set_index(['date','group']) # set indexes\n",
    "port_vwret=port_vwret.unstack(level=-1)\n",
    "port_vwret.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "(port_vwret+1).cumprod().plot(logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum anomaly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* here I copied and pasted our code from notebook 6\n",
    "\n",
    "* how do I change this code to work with stock level data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "del df['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ngroups=10\n",
    "\n",
    "DataUR=df.copy()\n",
    "DataUR=DataUR.set_index(['date'])\n",
    "\n",
    "DataUR['logret']=np.log(DataUR['ret']+1)\n",
    "temp=DataUR.groupby('permco')[['logret']].rolling(window=1).sum()\n",
    "temp =temp.reset_index()\n",
    "temp['cumret']=np.exp(temp['logret'])-1# transform back in geometric  returns\n",
    "\n",
    "\n",
    "temp = pd.merge(DataUR.reset_index(), temp[['permco','date','cumret']], how='left', on=['permco','date'])\n",
    "# merge the 12 month return signal back to the original database\n",
    "temp['mom']=temp.groupby('permco')['cumret'].shift(1)\n",
    "\n",
    "\n",
    "mom=temp.sort_values(['date','permco']) # sort by date and firm identifier \n",
    "mom=mom.dropna(subset=['mom'], how='any')# drop the row if any of these variables 'mom','ret','me' are missing\n",
    "mom['mom_group']=mom.groupby(['date'])['mom'].transform(lambda x: pd.qcut(x, ngroups, labels=False,duplicates='drop'))\n",
    "# create 10 groups each month. Assign membership accroding to the stock ranking in the distribution of trading signal \n",
    "#in a given month \n",
    "# transform in string the group names\n",
    "mom=mom.dropna(subset=['mom_group'], how='any')\n",
    "mom[['mom_group']]='m'+mom[['mom_group']].astype(int).astype(str)\n",
    "\n",
    "\n",
    "mom=mom[['date','permco','ret','mom_group']]\n",
    "mom=mom.sort_values(['permco','date']) # resort \n",
    "\n",
    "# To form equal weighted portfolios is enough to take means!\n",
    "port_vwret = mom.groupby(['date','mom_group']).mean()\n",
    "# row the different dates\n",
    "port_vwret = port_vwret.reset_index().rename(columns={0:'port_vwret'})# give a name to the new time-seires\n",
    "\n",
    "port_vwret.head(10)\n",
    "port_vwret=port_vwret.set_index(['date','mom_group']) # set indexes\n",
    "port_vwret=port_vwret.ret.unstack(level=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "(port_vwret+1).cumprod().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ngroups=10\n",
    "DataUR=DataR.unstack()\n",
    "DataUR=pd.DataFrame(DataUR)\n",
    "DataUR=DataUR.reset_index()\n",
    "DataUR.rename(columns={'level_0':'ind','level_1':'date',0:'R'},inplace=True)\n",
    "DataUR=DataUR.set_index(['date'])\n",
    "\n",
    "\n",
    "DataUR['logret']=np.log(DataUR['R']+1)\n",
    "temp=DataUR.groupby('ind')[['logret']].rolling(window=12).sum()\n",
    "temp =temp.reset_index()\n",
    "temp['cumret']=np.exp(temp['logret'])-1# transform back in geometric  returns\n",
    "\n",
    "\n",
    "temp = pd.merge(DataUR.reset_index(), temp[['ind','date','cumret']], how='left', on=['ind','date'])\n",
    "# merge the 12 month return signal back to the original database\n",
    "temp['mom']=temp.groupby('ind')['cumret'].shift(2)\n",
    "\n",
    "\n",
    "mom=temp.sort_values(['date','ind']) # sort by date and firm identifier \n",
    "mom=mom.dropna(subset=['mom'], how='any')# drop the row if any of these variables 'mom','ret','me' are missing\n",
    "mom['mom_group']=mom.groupby(['date'])['mom'].transform(lambda x: pd.qcut(x, ngroups, labels=False,duplicates='drop'))\n",
    "# create 10 groups each month. Assign membership accroding to the stock ranking in the distribution of trading signal \n",
    "#in a given month \n",
    "# transform in string the group names\n",
    "mom[['mom_group']]='m'+mom[['mom_group']].astype(int).astype(str)\n",
    "\n",
    "\n",
    "mom=mom[['date','ind','R','mom_group']]\n",
    "mom=mom.sort_values(['ind','date']) # resort \n",
    "\n",
    "# To form equal weighted portfolios is enough to take means!\n",
    "port_vwret = mom.groupby(['date','mom_group']).mean()\n",
    "# row the different dates\n",
    "port_vwret = port_vwret.reset_index().rename(columns={0:'port_vwret'})# give a name to the new time-seires\n",
    "port_vwret=port_vwret.set_index(['date','mom_group']) # set indexes\n",
    "port_vwret=port_vwret.unstack(level=-1) \n",
    "port_vwret.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# look at cumulative returns plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# look at annulaized means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# look at annualized Sharpe ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# look at winner minus looser: means, sharpe ratios, cumulative returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the actual code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def momreturns(data):\n",
    "    \n",
    "\n",
    "    _tmp_crsp = data[['permno','date','ret', 'me','vol','prc']].sort_values(['permno','date']).set_index('date')\n",
    "    _tmp_crsp['volume']=_tmp_crsp['vol']*_tmp_crsp['prc'].abs()*100/1e6 # in million dollars like me\n",
    "    ## Trading siginal construction\n",
    "    _tmp_crsp['ret']=_tmp_crsp['ret'].fillna(0)#replace missing return with 0\n",
    "    _tmp_crsp['logret']=np.log(1+_tmp_crsp['ret'])#transform in log returns\n",
    "\n",
    "    _tmp_cumret = _tmp_crsp.groupby(['permno'])['logret'].rolling(12, min_periods=7).sum()# sum last 12 month returns for each \n",
    "\n",
    "    \n",
    "    #stock,require there is a minium of 7 months\n",
    "    \n",
    "    _tmp_cumret = _tmp_cumret.reset_index()# reset index, needed to merged back below\n",
    "\n",
    "    _tmp_cumret['cumret']=np.exp(_tmp_cumret['logret'])-1# transform back in geometric  returns\n",
    "    _tmp_cumret = pd.merge(_tmp_crsp.reset_index(), _tmp_cumret[['permno','date','cumret']], how='left', on=['permno','date'])\n",
    "    # merge the 12 month return signal back to the original database\n",
    "\n",
    "    _tmp_cumret['mom']=_tmp_cumret.groupby('permno')['cumret'].shift(2) # lag the 12 month signal by two months\n",
    " \n",
    "    # You always have to lag by 1 month to make the signal tradable, that is if you are going to use the signal\n",
    "    #to construct a portfolio in the beggining of january/2018, you can only have returns in the signal up to Dec/2017\n",
    "    # we are lagging one more time, because the famous momentum strategy skips a month as well \n",
    "\n",
    "    # we also labeling mom the trading signal\n",
    "    mom=_tmp_cumret.sort_values(['date','permno']).drop_duplicates() # sort by date and firm identifier and drop in case there \n",
    "    #any duplicates (IT shouldn't be, in a given date for a given firm we can only have one row)\n",
    "\n",
    "    mom['w']=mom.groupby('permno')['me'].shift(1) # lag the market equity that we will use in our trading strategy to construct\n",
    "   \n",
    "    # value-weighted returns\n",
    "    mom=mom.dropna(subset=['mom','ret','w'], how='any')# drop the row if any of these variables 'mom','ret','me' are missing\n",
    "    mom['mom_group']=mom.groupby(['date'])['mom'].transform(lambda x: pd.qcut(x, 10, labels=False,duplicates='drop'))\n",
    "\n",
    "    # create 10 groups each month. Assign membership accroding to the stock ranking in the distribution of trading signal \n",
    "    #in a given month \n",
    "\n",
    "\n",
    "\n",
    "    # transform in string the group names\n",
    "    mom[['mom_group']]='m'+mom[['mom_group']].astype(int).astype(str)\n",
    "    \n",
    "    mom['date']=mom['date']+MonthEnd(0) #shift all the date to end of the month\n",
    "    mom=mom.sort_values(['permno','date']) # resort \n",
    "\n",
    "    # we now have the membership that will go in each portfolio\n",
    "    # withing a given portfolio we will simply use the firms market cap to value-weight the portfolio\n",
    "\n",
    "    # this function takes given date/set of firms given in group, and uses ret_name as the return series and\n",
    "    # weight_name as the variable to be used for weighting\n",
    "\n",
    "    # it returns, on number, the value weighted returns\n",
    "\n",
    "    def wavg(group, ret_name, weight_name):\n",
    "        d = group[ret_name]\n",
    "        w = group[weight_name]\n",
    "        try:\n",
    "            return (d * w).sum() / w.sum()\n",
    "        except ZeroDivisionError:\n",
    "            return np.nan\n",
    "    # We now simply have to  use the above function to construct the portfolio\n",
    "\n",
    "    # the code below applies the function in each date,mom_group group,\n",
    "    # so it applies the function to each of these subgroups of the data set, \n",
    "    # so it retursn one time-series for each mom_group, as it average the returns of\n",
    "    # all the firms in a given group in a given date\n",
    "    port_vwret = mom.groupby(['date','mom_group']).apply(wavg, 'ret','w')\n",
    "    port_vwret = port_vwret.reset_index().rename(columns={0:'port_vwret'})# give a name to the new time-seires\n",
    "    port_vwret=port_vwret.set_index(['date','mom_group']) # set indexes\n",
    "    port_vwret=port_vwret.unstack(level=-1) # unstack so we have in each column the different portfolios, and in each \n",
    "    port_vwret=port_vwret.port_vwret\n",
    "    \n",
    "    return port_vwret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mom=momreturns(crsp_m)\n",
    "mom.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a flexible code that allow us to modify our trading strategy\n",
    "\n",
    "1. strategy look back period\n",
    "2. Skip the first month?\n",
    "3. Weighting criteria: value weighted, equal weighted, volume weighted\n",
    "4. return transformation: sum? std? min? max?\n",
    "5. number of portfolios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# copy de code here and modify it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some insights on Implementation and Liquidity\n",
    "\n",
    "So we have taken prices as given and discussed quantitative allocation rules that overperform relative to a standard CAPM benchmark.\n",
    "\n",
    "An important question is wheter we would be able to trade at those prices if we tried to implement the strategy.\n",
    "\n",
    "Liquidity is the ease of trading a security.\n",
    "\n",
    "This can be quite complex and encompass many things, all of which impose a cost on those wishing to trade.\n",
    "\n",
    "Sources of illiquidity:\n",
    "\n",
    "1. Exogenous transactions costs = brokerage fees, order-processing costs, transaction taxes.\n",
    "\n",
    "2. Demand pressure = when need to sell quickly, sometimes a buyer is not immediately available (search costs).\n",
    "\n",
    "3. Inventory risk = if canâ€™t find a buyer will sell to a market maker who will later sell position.  But, since market maker faces future price changes, you must compensate him for this risk.\n",
    "\n",
    "4. Private information = concern over trading against an informed party (e.g., insider).  Need to be compensated for this. Private information can be about fundamentals or order flow\n",
    "\n",
    "Obviuosly a perfect answer to this question requires costly experimentation. It would require trading and measuring how prices change in reponse to your trading behavior.\n",
    "\n",
    "A much simpler approach is to measure the absorption capacity of the strategy.\n",
    "\n",
    "The idea is to measure how much of the trading volume of each stock you would \"use\" to implement the strategy for a given position size\n",
    "\n",
    "Specifically:\n",
    "\n",
    "$$UsedVolume_{i,t}=\\frac{Trading_{i,t}}{Volume_{i,t}}$$\n",
    "\n",
    "The idea is that if you trade a small share of the volume, you are likely to able to trade at the posted prices.\n",
    "\n",
    "\n",
    "To compute how much you need to trade you need to compare the desired weights in date $t$ with the weights you have in the end of date $t+1$.\n",
    "\n",
    "Before trading your weight in date $t+1$ is\n",
    "\n",
    "\n",
    "$$W_{i,t+1}(before trading)= \\frac{W_{i,t}^*(1+R_{i,t+1})}{(1+R_{t+1}^{strategy})}$$\n",
    "\n",
    "\n",
    "If the desired position in the stock is $$W_{i,t+1}^*$$, then\n",
    "\n",
    "$$UsedVolume_{i,t}=position\\frac{W_{i,t+1}^*-W_{i,t+1}(before trading)}{Volume_{i,t}}$$\n",
    "\n",
    "which is\n",
    "\n",
    "\n",
    "$$UsedVolume_{i,t}=\\frac{position}{Volume_{i,t}}\\left(W_{i,t+1}^*-\\frac{W_{i,t}^*(1+R_{i,t+1})}{1+R_{t+1}^{strategy}}\\right)$$\n",
    "\n",
    "\n",
    "$UsedVolume_{i,t}$ is a stock-time specific statistic. Implementability will depend of how high this quantity is across time and across stocks-- the lower the better. \n",
    "\n",
    "If it is very high--i.e. close to  1-- means that your position would require almost all volume in a particular stock. It doesn't mean that you wouldn't be able to trade, but likely prices would move against you (i.e., go up as you buy, do down as you sell)\n",
    "\n",
    "One vere conservative way of looking at it to look at the maximum of this statistic across stocks. This tell you the \"weakest\" link in your portfolio formation. \n",
    "\n",
    "The max statistic for  is the right one to look at if you are unwilling to deviate from your \"wish portfolio\".\n",
    "\n",
    "But the \"wish portfolio\" does not take into account transaction costs\n",
    "\n",
    "How can portfolios take into account trading costs to reduce total costs substantially?\n",
    "\n",
    "Can we change the portfolios to reduce trading costs without altering them significantly?\n",
    "\n",
    "One simple way of looking at this is to look at the  95/75/50 percentiles of the used volume distribution.\n",
    "\n",
    "If it declines steeply it might makes sense to avoid the to 5%/25% of the stocks that least liquid in you portfolio\n",
    "\n",
    "But as you deviate from the original portfolio you will have tracking error relative to the original strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def momreturns_w(data):\n",
    "    \n",
    "\n",
    "    _tmp_crsp = data[['permno','date','ret', 'me','vol','prc']].sort_values(['permno','date']).set_index('date')\n",
    "    _tmp_crsp['volume']=_tmp_crsp['vol']*_tmp_crsp['prc'].abs()*100/1e6 # in million dollars like me\n",
    "\n",
    "    _tmp_crsp['ret']=_tmp_crsp['ret'].fillna(0)#replace missing return with 0\n",
    "    _tmp_crsp['logret']=np.log(1+_tmp_crsp['ret'])#transform in log returns\n",
    "\n",
    "    _tmp_cumret = _tmp_crsp.groupby(['permno'])['logret'].rolling(12, min_periods=12).sum()# sum last 12 month returns for each \n",
    "    _tmp_cumret = _tmp_cumret.reset_index()# reset index, needed to merged back below\n",
    "\n",
    "    _tmp_cumret['cumret']=np.exp(_tmp_cumret['logret'])-1# transform back in geometric  returns\n",
    "    _tmp_cumret = pd.merge(_tmp_crsp.reset_index(), _tmp_cumret[['permno','date','cumret']], how='left', on=['permno','date'])\n",
    "    # merge the 12 month return signal back to the original database\n",
    "\n",
    "    _tmp_cumret['mom']=_tmp_cumret.groupby('permno')['cumret'].shift(2) # lag the 12 month signal by two months\n",
    " \n",
    " \n",
    "    mom=_tmp_cumret.sort_values(['date','permno']).drop_duplicates() # sort by date and firm identifier and drop in case there \n",
    "    #any duplicates (IT shouldn't be, in a given date for a given firm we can only have one row)\n",
    "\n",
    "    mom['w']=mom.groupby('permno')['me'].shift(1) # lag the market equity that we will use in our trading strategy to construct\n",
    "   \n",
    "    # value-weighted returns\n",
    "    mom=mom.dropna(subset=['mom','ret','w'], how='any')# drop the row if any of these variables 'mom','ret','me' are missing\n",
    "    mom['mom_group']=mom.groupby(['date'])['mom'].transform(lambda x: pd.qcut(x, 10, labels=False,duplicates='drop'))\n",
    "    mom[['mom_group']]='m'+mom[['mom_group']].astype(int).astype(str)\n",
    "    \n",
    "    mom['date']=mom['date']+MonthEnd(0) #shift all the date to end of the month\n",
    "    mom=mom.sort_values(['permno','date']) # resort \n",
    "\n",
    "\n",
    "    def wavg(group, ret_name, weight_name):\n",
    "        d = group[ret_name]\n",
    "        w = group[weight_name]\n",
    "        try:\n",
    "            group['Wght']=(w) / w.sum()\n",
    "            return group[['date','permno','Wght']]\n",
    "        except ZeroDivisionError:\n",
    "            return np.nan\n",
    "    # We now simply have to  use the above function to construct the portfolio\n",
    "\n",
    "    # the code below applies the function in each date,mom_group group,\n",
    "    # so it applies the function to each of these subgroups of the data set, \n",
    "    # so it retursn one time-series for each mom_group, as it average the returns of\n",
    "    # all the firms in a given group in a given date\n",
    "    \n",
    "    weights = mom.groupby(['date','mom_group']).apply(wavg, 'ret','me') \n",
    "# merge back\n",
    "    weights=mom.merge(weights,on=['date','permno'])\n",
    "    weights=weights.sort_values(['date','permno'])\n",
    "\n",
    "\n",
    "    weights['mom_group_lead']=weights.groupby('permno').mom_group.shift(-1)\n",
    "    weights['Wght_lead']=weights.groupby('permno').Wght.shift(-1)\n",
    "    weights=weights.sort_values(['permno','date'])\n",
    "    \n",
    "    def wavg(group, ret_name, weight_name):\n",
    "        d = group[ret_name]\n",
    "        w = group[weight_name]\n",
    "        try:\n",
    "            return (d * w).sum() / w.sum()\n",
    "        except ZeroDivisionError:\n",
    "            return np.nan\n",
    "    \n",
    "    port_vwret = mom.groupby(['date','mom_group']).apply(wavg, 'ret','w')\n",
    "    port_vwret = port_vwret.reset_index().rename(columns={0:'port_vwret'})# give a name to the new time-seires\n",
    "   \n",
    "    \n",
    "    weights=weights.merge(port_vwret,how='left',on=['date','mom_group'])\n",
    "    \n",
    "    port_vwret=port_vwret.set_index(['date','mom_group']) # set indexes\n",
    "    port_vwret=port_vwret.unstack(level=-1) # unstack so we have in each column the different portfolios, and in each \n",
    "    port_vwret=port_vwret.port_vwret\n",
    "    return port_vwret, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "port_ret, wght=momreturns_w(crsp_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wght.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# construct dollars of each stock that need to be bought (sold) at date t+1\n",
    "mom=wght.copy()\n",
    "mom['trade']=mom.Wght_lead*(mom.mom_group==mom.mom_group_lead)-mom.Wght*(1+mom.ret)/(1+mom.port_vwret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# per dollar of position how much do you have to trade every month?\n",
    "mom.groupby(['date','mom_group']).trade.apply(lambda x:x.abs().sum()).loc[:,'m1'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# lets look at the most illiquid stocks, the stocks that I will likely have most trouble trading\n",
    "# lets start by normalizing the amount of trade per stock volume\n",
    "\n",
    "mom['tradepervol']=mom.trade/mom.volume\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# this allow us to study how much of the volume of each stock I will be \"using\"\n",
    "# lets choose a position size, here in millions of dollars , because that is the normalization we used for the volume data\n",
    "Position=1e3 #(1e3 means one billion dollars)\n",
    "threshold=0.05\n",
    "(Position*(mom.groupby(['date','mom_group']).tradepervol.quantile(threshold).loc[:,'m9'])).plot()\n",
    "(Position*(mom.groupby(['date','mom_group']).tradepervol.quantile(1-threshold).loc[:,'m9'])).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tracking error\n",
    "\n",
    "Let $$W^{wishportfolio}_t$$ and $$W^{Implementationportfolio}_t$$ weights , then consider the following regression\n",
    "\n",
    "$$W^{Implementationportfolio}_tR_{t+1}=\\alpha+\\beta W^{wishportfolio}_tR_{t+1}+\\epsilon_{t+1}$$\n",
    "\n",
    "a good implementation portfolio has $\\beta=1$ and $\\sigma(\\epsilon)$ and $\\alpha\\approx 0$.\n",
    "\n",
    "So one can think of $|\\beta-1|$, $\\sigma(\\epsilon)$ , and $\\alpha$ as three dimensions of tracking error.\n",
    "\n",
    "The $\\beta$ dimension can be more easily correted by levering up and down the tracking portoflio (of possible)\n",
    "\n",
    "The $\\sigma(\\epsilon)$ can only be corrected by simply making the implemenetation portoflio more similar to the wish portfolio. The cost of this is not obvious. Really depends how this tracking error relates to other stuff in your portfolio.\n",
    "\n",
    "$\\alpha$ is the important part. The actual cost that you expect to pay to deviate from the wish portfolio\n",
    "\n",
    "------------------------------\n",
    "\n",
    "In the industry people typicall refer to tracking error as simply\n",
    "\n",
    "$$\\sigma(W^{Implementationportfolio}_tR_{t+1}- W^{wishportfolio}_tR_{t+1})$$\n",
    "\n",
    "The volatility of a portfolio that goes long the implementation portfolio and shorts the wish portfolio.\n",
    "\n",
    "This mixes together $|\\beta-1|$, $\\sigma(\\epsilon)$ and completely ignores $\\alpha$\n",
    "\n",
    "\n",
    "In the end the Implementation portfolio is chosen by trading off  trading costs (market impact) and opportunity cost (tracking error).\n",
    "\n",
    "So you can simply construct strategies that avoid these 5% less liquid stocks, and see how much your tracking error increases and whether these tracking errors are worth the reduction in trading costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to construct an Implementation portfolio?\n",
    "\n",
    "- A simple strategy: weight by trading volume -> this make sure that you use the same amount of trading volume across all your positions\n",
    "\n",
    "\n",
    "- Harder to implement: do not buy stocks that are illiquid now or likely to be illiquid next period. Amounts to add another signal interected to the momentum signal. Only buy if illiquid signal not too strong.\n",
    "\n",
    "How to change our code to implement the volume-weighted approach?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def momreturns(data,lookback=12,nmonthsskip=1,wghtvar='me',returntransf='sum',nportfolios=10):\n",
    "    \n",
    "\n",
    "    _tmp_crsp = data[['permno','date','ret', 'me','vol','prc']].sort_values(['permno','date']).set_index('date')\n",
    "    _tmp_crsp['volume']=_tmp_crsp['vol']*_tmp_crsp['prc'].abs()*100/1e6 # in million dollars like me\n",
    "    ## Trading siginal construction\n",
    "    _tmp_crsp['ret']=_tmp_crsp['ret'].fillna(0)#replace missing return with 0\n",
    "    _tmp_crsp['logret']=np.log(1+_tmp_crsp['ret'])#transform in log returns\n",
    "    if returntransf=='sum':\n",
    "        _tmp_cumret = _tmp_crsp.groupby(['permno'])['logret'].rolling(lookback, min_periods=np.min([7,lookback])).sum()# sum last 12 month returns for each \n",
    "    elif returntransf=='std':\n",
    "        _tmp_cumret = _tmp_crsp.groupby(['permno'])['logret'].rolling(lookback, min_periods=np.min([7,lookback])).std()# sum last 12 month returns for each \n",
    "    elif returntransf=='min':\n",
    "        _tmp_cumret = _tmp_crsp.groupby(['permno'])['logret'].rolling(lookback, min_periods=np.min([7,lookback])).min()# sum last 12 month returns for each \n",
    "    elif returntransf=='max':\n",
    "        _tmp_cumret = _tmp_crsp.groupby(['permno'])['logret'].rolling(lookback, min_periods=np.min([7,lookback])).max()# sum last 12 month returns for each \n",
    " \n",
    "    \n",
    "    \n",
    "    #stock,require there is a minium of 7 months\n",
    "    \n",
    "    _tmp_cumret = _tmp_cumret.reset_index()# reset index, needed to merged back below\n",
    "\n",
    "    _tmp_cumret['cumret']=np.exp(_tmp_cumret['logret'])-1# transform back in geometric  returns\n",
    "    _tmp_cumret = pd.merge(_tmp_crsp.reset_index(), _tmp_cumret[['permno','date','cumret']], how='left', on=['permno','date'])\n",
    "    # merge the 12 month return signal back to the original database\n",
    "\n",
    "    _tmp_cumret['mom']=_tmp_cumret.groupby('permno')['cumret'].shift(1+nmonthsskip) # lag the 12 month signal by two months\n",
    " \n",
    "    # You always have to lag by 1 month to make the signal tradable, that is if you are going to use the signal\n",
    "    #to construct a portfolio in the beggining of january/2018, you can only have returns in the signal up to Dec/2017\n",
    "    # we are lagging one more time, because the famous momentum strategy skips a month as well \n",
    "\n",
    "    # we also labeling mom the trading signal\n",
    "    mom=_tmp_cumret.sort_values(['date','permno']).drop_duplicates() # sort by date and firm identifier and drop in case there \n",
    "    #any duplicates (IT shouldn't be, in a given date for a given firm we can only have one row)\n",
    "    if wghtvar=='ew':\n",
    "        mom['w']=1\n",
    "    else:\n",
    "        mom['w']=mom.groupby('permno')[wghtvar].shift(1) # lag the market equity that we will use in our trading strategy to construct\n",
    "   \n",
    "    # value-weighted returns\n",
    "    mom=mom.dropna(subset=['mom','ret','me'], how='any')# drop the row if any of these variables 'mom','ret','me' are missing\n",
    "    mom['mom_group']=mom.groupby(['date'])['mom'].transform(lambda x: pd.qcut(x, nportfolios, labels=False,duplicates='drop'))\n",
    "\n",
    "    # create 10 groups each month. Assign membership accroding to the stock ranking in the distribution of trading signal \n",
    "    #in a given month \n",
    "\n",
    "\n",
    "\n",
    "    # transform in string the group names\n",
    "    mom[['mom_group']]='m'+mom[['mom_group']].astype(int).astype(str)\n",
    "    \n",
    "    mom['date']=mom['date']+MonthEnd(0) #shift all the date to end of the month\n",
    "    mom=mom.sort_values(['permno','date']) # resort \n",
    "\n",
    "    # we now have the membership that will go in each portfolio\n",
    "    # withing a given portfolio we will simply use the firms market cap to value-weight the portfolio\n",
    "\n",
    "    # this function takes given date/set of firms given in group, and uses ret_name as the return series and\n",
    "    # weight_name as the variable to be used for weighting\n",
    "\n",
    "    # it returns, on number, the value weighted returns\n",
    "\n",
    "    def wavg(group, ret_name, weight_name):\n",
    "        d = group[ret_name]\n",
    "        w = group[weight_name]\n",
    "        try:\n",
    "            return (d * w).sum() / w.sum()\n",
    "        except ZeroDivisionError:\n",
    "            return np.nan\n",
    "    # We now simply have to  use the above function to construct the portfolio\n",
    "\n",
    "    # the code below applies the function in each date,mom_group group,\n",
    "    # so it applies the function to each of these subgroups of the data set, \n",
    "    # so it retursn one time-series for each mom_group, as it average the returns of\n",
    "    # all the firms in a given group in a given date\n",
    "    port_vwret = mom.groupby(['date','mom_group']).apply(wavg, 'ret','w')\n",
    "    port_vwret = port_vwret.reset_index().rename(columns={0:'port_vwret'})# give a name to the new time-seires\n",
    "    port_vwret=port_vwret.set_index(['date','mom_group']) # set indexes\n",
    "    port_vwret=port_vwret.unstack(level=-1) # unstack so we have in each column the different portfolios, and in each \n",
    "    port_vwret=port_vwret.port_vwret\n",
    "    \n",
    "    return port_vwret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "momportfolios=momreturns(crsp_m,lookback=12,nmonthsskip=1,wghtvar='me',returntransf='sum',nportfolios=10)\n",
    "momportfoliosvol=momreturns(crsp_m,lookback=12,nmonthsskip=1,wghtvar='vol',returntransf='sum',nportfolios=10)\n",
    "momportfolios=momportfolios.merge(momportfoliosvol,left_index=True,right_index=True,suffixes=['_me','_vol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "momportfolios[['m9_me','m9_vol']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# lets look at it's tracking error\n",
    "y=momportfolios['m9_vol']\n",
    "x=momportfolios['m9_me']\n",
    "x=sm.add_constant(x)\n",
    "results = sm.OLS(y,x).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- The beta difference 1.17 vs 1 can be adjusted by taking a smaller position on the implementation portfolio\n",
    "\n",
    "- The alpha is the actual tracking error loss. How much you expect to loose.\n",
    "\n",
    "- In this case it is economically quite large -0.0036 vs $\\beta E[R]$ of 0.0045\n",
    "\n",
    "- One calculation that people do is to see how much you are getting for your momentum exposure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "[momportfolios['m9_vol'].mean()/results.params[1]*12,momportfolios['m9_me'].mean()*12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very large. A decay of about 50% in the premium that you earn for trading momentum\n",
    "\n",
    "But be careful to not over interpret this. We are working today with a very short sample, less than 20 years, for average returns tests that is not much at all.\n",
    "\n",
    "But beta/residulas are well measured even in fairly short samples.\n",
    "\n",
    "\n",
    "In addition to that you also have to eat the strategy residual risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "[results.resid.std(),momportfolios['m9_me'].std()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sizable, about 50% of the original strategy volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How our strategies compare with the momentum factor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.dropbox.com/s/9346pp2iu5prv8s/MonthlyFactors.csv?dl=1\"\n",
    "Factors = pd.read_csv(url,index_col=0, \n",
    "                         parse_dates=True,na_values=-99)\n",
    "Factors=Factors/100\n",
    "\n",
    "Factors=Factors.iloc[:,0:5]\n",
    "Factors['MKT']=Factors['MKT']-Factors['RF']\n",
    "Factors=Factors.drop('RF',axis=1)\n",
    "Factors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "momportfolios=momportfolios.merge(Factors,left_index=True,right_index=True)\n",
    "momportfolios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# things to look at:\n",
    "\n",
    "#y=momportfolios['m9_me']\n",
    "#y=momportfolios['m9_vol']\n",
    "#y=momportfolios['m9_me']-momportfolios['m0_me']\n",
    "\n",
    "\n",
    "y=momportfolios['m9_me']-momportfolios['m0_me']\n",
    "x=momportfolios[['MKT','Mom']]\n",
    "x=sm.add_constant(x)\n",
    "results = sm.OLS(y,x).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why such a large differcenc even for the strategy that follows the Momentum strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The devil is in the details: Data Cleaning\n",
    "\n",
    "Eugene Fama always would tell his students: Garbage in, Garbage out\n",
    "\n",
    "He was also able to look at a Table of a regression, or summary statistics and immediately tell that the person had \n",
    "done something different with the data when cleaning it.\n",
    "\n",
    "\n",
    "When working with raw data, it is really important to undergo careful work to check if the underlying data is indeed real\n",
    "\n",
    "CRSP dataset is very clean already, but if you are not careful your analysis will pick up features of the data are not real\n",
    "\n",
    "- Load on microstructure effects- bid-ask bounces\n",
    "\n",
    "- Load on tiny/peny stocks\n",
    "\n",
    "- Load on stocks that are outside of your investment mandate (for example stocks that do not trade in the US)\n",
    "\n",
    "- Load on other financial instruments (closed-end funds, convertible, prefered shares..)\n",
    "\n",
    "\n",
    "For example, Daniel and Moskowitz impose the following data requriements\n",
    "\n",
    "\n",
    "(a). CRSP Sharecode 10 or 11. (i.e, common stocks; no ADRs)\n",
    "\n",
    "(b). CRSP exchange code of 1, 2 or 3 (NYSE, AMEX or NASDAQ)\n",
    "\n",
    "(c). Must have price at date t-13\n",
    "\n",
    "(d). Must have return at date t-2\n",
    "\n",
    "(e). Must have market cap at date t-1\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
